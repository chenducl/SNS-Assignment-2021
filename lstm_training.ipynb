{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ef9226f5f281bed155f6877abe961f8893e93036b236b1a7e5395de73a55139a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Model Training\n",
    "\n",
    "- Single Layer LSTM\n",
    "- Multiple Layers LSTM\n",
    "- LSTM with Attention"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessities\n",
    "import os\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import GRU, LSTM, Dense, Dropout, Activation, Input\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readning in the dataset\n",
    "dataset_csv = pd.read_csv('./data/dataset.csv', index_col=0)\n",
    "dataset_rolling_csv = pd.read_csv('./data/dataset_rolling.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, dataset, feature_cols, label_cols, timestamp=14, \n",
    "                        batch_size=1, test_size=0.03):\n",
    "        ''' Initialize the dataset\n",
    "        :param dataset: Dataframe of the dataset\n",
    "        :param feature_cols: list of feature names,\n",
    "            e.g. ['dates', 'vaccinations'] means using dates and vaccinations as features\n",
    "        :param label_cols: list of prediction targets,\n",
    "            e.g. ['confirmed'] means using confirmed data as labels\n",
    "        :param timestamp: timestamp in LSTM model\n",
    "        :param test_size: the ratio of test data in the dataset\n",
    "        '''\n",
    "        self.dataset = dataset\n",
    "        self.feature_cols = feature_cols\n",
    "        self.label_cols = label_cols\n",
    "        self.timestamp = timestamp\n",
    "        self.batch_size = batch_size\n",
    "        self.test_size = test_size\n",
    "        # Split features and labels\n",
    "        self.features = self.dataset[feature_cols]\n",
    "        self.labels = self.dataset[label_cols]\n",
    "        # Normalize the dataset using MinMaxScaler before training\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.dataset.loc[:, dataset.columns != 'dates'] = self.scaler.fit_transform(dataset.loc[:, dataset.columns != 'dates'])\n",
    "        # Split the dataset\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(\n",
    "                self.features, self.labels, test_size=self.test_size, shuffle=False)\n",
    "    \n",
    "    def get_training_set(self):\n",
    "        ''' Return the time-series generator used for training\n",
    "        '''\n",
    "        return TimeseriesGenerator(self.x_train.to_numpy(), self.y_train.to_numpy(),\n",
    "                length=self.timestamp, batch_size=self.batch_size)\n",
    "    \n",
    "    def get_test_set(self):\n",
    "        ''' Return the time-series generator used for testing\n",
    "        '''\n",
    "        # Add overlapping points to test set\n",
    "        x_test = pd.concat([self.x_train[-self.timestamp:], self.x_test])\n",
    "        y_test = pd.concat([self.y_train[-self.timestamp:], self.y_test])\n",
    "        return TimeseriesGenerator(x_test.to_numpy(), y_test.to_numpy(),\n",
    "                length=self.timestamp, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel():\n",
    "    ''' Other models are derived by inheritance from BaseModel.\n",
    "    '''\n",
    "    def __init__(self, dataset, model_path, output_dim, epochs,\n",
    "                        verbose, loss, optimizer, dropout, **args):\n",
    "        ''' Initialize the model parameters\n",
    "        param dataset: dataset Dataframe\n",
    "        param output_dim: output dimension\n",
    "        param epochs: training epochs\n",
    "        param verbose: verbose level of logging\n",
    "        param loss: loss function for training\n",
    "        param optimizer: optimizer for training\n",
    "        '''\n",
    "        self.dataset = dataset\n",
    "        self.model_path = model_path\n",
    "        self.output_dim = output_dim\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def read_model(self):\n",
    "        ''' Load model from the given path\n",
    "        '''\n",
    "        self.model = load_model(self.model_path)\n",
    "\n",
    "    def train(self):\n",
    "        ''' Training\n",
    "        '''\n",
    "        training_set = self.dataset.get_training_set()\n",
    "        self.history = self.model.fit(training_set, epochs=self.epochs, verbose=self.verbose)\n",
    "    \n",
    "    def save_model(self):\n",
    "        ''' Save model to path\n",
    "        '''\n",
    "        self.model.save(self.model_path)\n",
    "    \n",
    "    def predict(self, input):\n",
    "        return self.model.predict(input)\n",
    "\n",
    "    def evaluate(self):\n",
    "        return self.model.evaluate(self.dataset.get_test_set())\n",
    "    \n",
    "    def plot(self, title=None):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        if title is None:\n",
    "            plt.title('Labels and Predictions on ' + ', '.join(self.dataset.label_cols))\n",
    "        else:\n",
    "            plt.title(title)\n",
    "        len_train = len(self.dataset.y_train)\n",
    "        len_test = len(self.dataset.y_test)\n",
    "        plt.plot(range(0, len_train), self.dataset.y_train, label='y_train')\n",
    "        plt.plot(range(self.dataset.timestamp, len_train), self.model.predict(self.dataset.get_training_set()), label='pred_train')\n",
    "        plt.plot(range(len_train, len_train+len_test), self.dataset.y_test, label='y_test')\n",
    "        plt.plot(range(len_train, len_train+len_test), self.model.predict(self.dataset.get_test_set()), label='pred_test')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(BaseModel):\n",
    "    ''' LSTM models structures and implementation.\n",
    "    '''\n",
    "    def __init__(self, dataset=None, model_path='', model_type='',\n",
    "                        output_dim=128, epochs=400, verbose=1,\n",
    "                        loss='mse', optimizer='adam', dropout=0.2,\n",
    "                        activation=None, share_attention=False, **args):\n",
    "        ''' Initialize the model parameters\n",
    "        param dataset: dataset Dataframe\n",
    "        param output_dim: output dimension\n",
    "        param epochs: training epochs\n",
    "        param verbose: verbose level of logging\n",
    "        param loss: loss function for training\n",
    "        param optimizer: optimizer for training\n",
    "        '''\n",
    "        super(LSTMModel, self).__init__(dataset, model_path,\n",
    "                                        output_dim, epochs, verbose,\n",
    "                                        loss, optimizer, dropout, **args)\n",
    "        self.share_attention = share_attention\n",
    "        self.activation = activation\n",
    "    \n",
    "    def attention_block(input):\n",
    "        ''' Attention block\n",
    "        '''\n",
    "        # (batch_size, time_steps, input_dim)\n",
    "        input_dim = int(input.shape[2])\n",
    "        # Transpose with Permute\n",
    "        a = Permute((2, 1))(inputs)\n",
    "        a = Reshape((input_dim, self.dataset.timestamp))(a)\n",
    "        a = Dense(self.dataset.timestamp, activation='softmax')(a)\n",
    "        if self.share_attention:\n",
    "            a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "            a = RepeatVector(input_dim)(a)\n",
    "        a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "        return Multiply()([inputs, a_probs])\n",
    "\n",
    "    def construct_single_layer_lstm(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(units=self.output_dim, input_shape=(self.dataset.timestamp, len(self.dataset.feature_cols))))\n",
    "        # self.model.add(Dropout(self.dropout))\n",
    "        self.model.add(Dense(units=len(self.dataset.label_cols)))\n",
    "        # Activation Functions\n",
    "        if self.activation != None:\n",
    "            self.model.add(Activation(self.activation))\n",
    "        # MSE loss and adam optimizer\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "\n",
    "    def construct_multi_layer_lstm(self):\n",
    "        self.model = Sequential()\n",
    "        # Three-layer LSTM with Dropout\n",
    "        self.model.add(LSTM(units=self.output_dim, return_sequences=True, input_shape=(self.dataset.timestamp, len(self.dataset.feature_cols))))\n",
    "        # self.model.add(Dropout(self.dropout))\n",
    "        self.model.add(LSTM(units=self.output_dim, return_sequences=True))\n",
    "        # self.model.add(Dropout(self.dropout))\n",
    "        self.model.add(LSTM(units=self.output_dim, return_sequences=False))\n",
    "        # self.model.add(Dropout(self.dropout))\n",
    "        # Dense Layer\n",
    "        self.model.add(Dense(units=len(self.dataset.label_cols)))\n",
    "        # Activation Functions\n",
    "        if self.activation != None:\n",
    "            self.model.add(Activation(self.activation))\n",
    "        # MSE loss and adam optimizer\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "\n",
    "    def construct_attention_lstm(self):\n",
    "        inputs = Input(shape=(self.dataset.timestamp, len(self.dataset.feature_cols)))\n",
    "        attention = self.attention_block(input)\n",
    "        attention = LSTM(units=self.output_dim, return_sequences=False)(attention)\n",
    "        output = Dense(1)(attention)\n",
    "        if self.activation != None:\n",
    "            self.model.add(Activation(self.activation))\n",
    "        self.model = Model(input=[inputs], output=output)\n",
    "\n",
    "    def construct_lstm_attention(self):\n",
    "        # Inputs Layer\n",
    "        inputs = Input(shape=(self.dataset.timestamp, len(self.dataset.feature_cols)))\n",
    "        # Single LSTM Layer\n",
    "        lstm = LSTM(units=self.output_dim, return_sequences=True)(inputs)\n",
    "        # Attention Block\n",
    "        attention = attention_block(lstm)\n",
    "        # Flatten to connect with Dense Layer\n",
    "        attention = Flatten()(attention)\n",
    "        output = Dense(1)(attention)\n",
    "        if self.activation != None:\n",
    "            self.model.add(Activation(self.activation))\n",
    "        self.model = Model(input=[inputs], output=output)\n",
    "\n",
    "    def construct_model(self, model_type='single_layer_lstm'):\n",
    "        ''' Build model from scratch\n",
    "        param model_type:\n",
    "            - single_layer_lstm\n",
    "            - multi_layer_lstm\n",
    "            - attention_lstm\n",
    "            - lstm_attention\n",
    "            - transformer\n",
    "        '''\n",
    "        if model_type is 'single_layer_lstm':\n",
    "            self.construct_single_layer_lstm()\n",
    "        elif model_type is 'multi_layer_lstm':\n",
    "            self.construct_multi_layer_lstm()\n",
    "        elif model_type is 'attention_lstm':\n",
    "            self.construct_attention_lstm()\n",
    "        elif model_type is 'lstm_attention':\n",
    "            self.construct_lstm_attention()\n",
    "        else:\n",
    "            raise(f'model_type {moel_type} Not Implemented!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_CONFIGS = {\n",
    "    'dataset': None,\n",
    "    'model_path': '',\n",
    "    'output_dim': 128,\n",
    "    'epochs': 400,\n",
    "    'verbose': 2,\n",
    "    'loss': 'mse',\n",
    "    'optimizer': 'adam',\n",
    "    'dropout': 0.2,\n",
    "    'share_attention': False,\n",
    "}\n"
   ]
  },
  {
   "source": [
    "## Activation Experiment\n",
    "\n",
    "Test on 4 activation functions in LSTM, including:\n",
    "\n",
    "- sigmoid\n",
    "- tanh\n",
    "- relu\n",
    "- linear"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataset\n",
    "features = ['dates', 'people_vaccinated', 'mobility', 'hosp_patients', 'icu_patients', 'total_tests', 'median_age']\n",
    "labels = ['confirmed']\n",
    "dataset = Dataset(dataset_csv, feature_cols=features, label_cols=labels)\n",
    "dataset_rolling = Dataset(dataset_rolling_csv, feature_cols=features, label_cols=labels)\n",
    "\n",
    "model_types = ['single_layer_lstm', 'multi_layer_lstm', 'attention_lstm', 'lstm_attention']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enumerating and training\n",
    "for model_type in model_types:\n",
    "    for activation in ['sigmoid', 'tanh', 'relu', 'linear']:\n",
    "        configs = {\n",
    "            'dataset': dataset_rolling,\n",
    "            'model_path': f\"./models/{model_type}_{'_'.join(labels)}_{activation}\",\n",
    "            'model_type': model_type,\n",
    "            'activation': activation,\n",
    "        }\n",
    "        print(f'Start training {model_type}')\n",
    "        model = LSTMModel(**configs)\n",
    "        model.construct_model()\n",
    "        model.train()\n",
    "        model.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0013\n",
      "13/13 [==============================] - 1s 2ms/step - loss: 0.0017 \n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.9586\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 0.0023\n"
     ]
    }
   ],
   "source": [
    "model_type = 'single_layer_lstm'\n",
    "activations = ['sigmoid', 'tanh', 'relu', 'linear']\n",
    "loss = []\n",
    "for activation in activations:\n",
    "    configs = {\n",
    "        'dataset': dataset_rolling,\n",
    "        'model_path': f\"./models/{model_type}_{'_'.join(labels)}_{activation}\",\n",
    "        'model_type': model_type,\n",
    "        'activation': activation,\n",
    "    }\n",
    "    model = LSTMModel(**configs)\n",
    "    model.read_model()\n",
    "    # model.plot(f'Confirmed Cases Forcasting with Activation: {activation}')\n",
    "    loss.append(model.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'single_layer_lstm'\n",
    "activation = 'sigmoid'\n",
    "configs = {\n",
    "    'dataset': dataset_rolling,\n",
    "    'model_path': f\"./models/{model_type}_{'_'.join(labels)}_{activation}\",\n",
    "    'model_type': model_type,\n",
    "    'activation': activation,\n",
    "}\n",
    "model = LSTMModel(**configs)\n",
    "model.read_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "type(model.model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type in model_types:\n",
    "    configs = {\n",
    "        'dataset': dataset,\n",
    "        'model_path': f\"./models/{model_type}_{'_'.join(labels)}\",\n",
    "        'model_type': model_type,\n",
    "    }\n",
    "    model = LSTMModel(**configs)\n",
    "    model.read_model()\n",
    "    model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type in model_types:\n",
    "    configs = {\n",
    "        'dataset': dataset,\n",
    "        'model_path': f\"./models/{model_type}_{'_'.join(labels)}\",\n",
    "        'model_type': model_type,\n",
    "    }\n",
    "    model = LSTMModel(**configs)\n",
    "    model.read_model()\n",
    "    model.plot()"
   ]
  }
 ]
}