{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ef9226f5f281bed155f6877abe961f8893e93036b236b1a7e5395de73a55139a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Model Training\n",
    "\n",
    "- Single Layer LSTM\n",
    "- Multiple Layers LSTM\n",
    "- LSTM with Attention"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessities\n",
    "import os\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import GRU, LSTM, Dense, Dropout, Activation, Input\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, dataset, feature_cols, label_cols, timestamp=14, \n",
    "                        batch_size=1, test_size=0.03):\n",
    "        ''' Initialize the dataset\n",
    "        :param dataset: Dataframe of the dataset\n",
    "        :param feature_cols: list of feature names,\n",
    "            e.g. ['dates', 'vaccinations'] means using dates and vaccinations as features\n",
    "        :param label_cols: list of prediction targets,\n",
    "            e.g. ['confirmed'] means using confirmed data as labels\n",
    "        :param timestamp: timestamp in LSTM model\n",
    "        :param test_size: the ratio of test data in the dataset\n",
    "        '''\n",
    "        self.dataset = dataset\n",
    "        self.feature_cols = feature_cols\n",
    "        self.label_cols = label_cols\n",
    "        self.timestamp = timestamp\n",
    "        self.batch_size = batch_size\n",
    "        self.test_size = test_size\n",
    "        # Normalize the dataset using MinMaxScaler before training\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.dataset.loc[:, dataset.columns != 'dates'] = self.scaler.fit_transform(dataset.loc[:, dataset.columns != 'dates'])\n",
    "        # Split features and labels\n",
    "        self.features = self.dataset[feature_cols]\n",
    "        self.labels = self.dataset[label_cols]\n",
    "        # Split the dataset\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(\n",
    "                self.features, self.labels, test_size=self.test_size, shuffle=False)\n",
    "    \n",
    "    def get_training_set(self):\n",
    "        ''' Return the time-series generator used for training\n",
    "        '''\n",
    "        return TimeseriesGenerator(self.x_train.to_numpy(), self.y_train.to_numpy(),\n",
    "                length=self.timestamp, batch_size=self.batch_size)\n",
    "    \n",
    "    def get_test_set(self):\n",
    "        ''' Return the time-series generator used for testing\n",
    "        '''\n",
    "        # Add overlapping points to test set\n",
    "        x_test = pd.concat([self.x_train[-self.timestamp:], self.x_test])\n",
    "        y_test = pd.concat([self.y_train[-self.timestamp:], self.y_test])\n",
    "        return TimeseriesGenerator(x_test.to_numpy(), y_test.to_numpy(),\n",
    "                length=self.timestamp, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel():\n",
    "    ''' Other models are derived by inheritance from BaseModel.\n",
    "    '''\n",
    "    def __init__(self, dataset, model_path, output_dim, epochs,\n",
    "                        verbose, loss, optimizer, dropout, **args):\n",
    "        ''' Initialize the model parameters\n",
    "        param dataset: dataset Dataframe\n",
    "        param output_dim: output dimension\n",
    "        param epochs: training epochs\n",
    "        param verbose: verbose level of logging\n",
    "        param loss: loss function for training\n",
    "        param optimizer: optimizer for training\n",
    "        '''\n",
    "        self.dataset = dataset\n",
    "        self.model_path = model_path\n",
    "        self.output_dim = output_dim\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def read_model(self):\n",
    "        ''' Load model from the given path\n",
    "        '''\n",
    "        self.model = load_model(self.model_path)\n",
    "\n",
    "    def train(self):\n",
    "        ''' Training\n",
    "        '''\n",
    "        training_set = self.dataset.get_training_set()\n",
    "        self.history = self.model.fit(training_set, epochs=self.epochs, verbose=self.verbose)\n",
    "    \n",
    "    def save_model(self):\n",
    "        ''' Save model to path\n",
    "        '''\n",
    "        self.model.save(self.model_path)\n",
    "    \n",
    "    def predict(self, input):\n",
    "        return self.model.predict(input)\n",
    "\n",
    "    def evaluate(self):\n",
    "        return self.model.evaluate(self.dataset.get_test_set())\n",
    "    \n",
    "    def plot(self, title=None):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        if title is None:\n",
    "            plt.title('Labels and Predictions on ' + ', '.join(self.dataset.label_cols))\n",
    "        else:\n",
    "            plt.title(title)\n",
    "        len_train = len(self.dataset.y_train)\n",
    "        len_test = len(self.dataset.y_test)\n",
    "        plt.plot(range(0, len_train), self.dataset.y_train, label='y_train')\n",
    "        plt.plot(range(self.dataset.timestamp, len_train), self.model.predict(self.dataset.get_training_set()), label='pred_train')\n",
    "        plt.plot(range(len_train, len_train+len_test), self.dataset.y_test, label='y_test')\n",
    "        plt.plot(range(len_train, len_train+len_test), self.model.predict(self.dataset.get_test_set()), label='pred_test')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(BaseModel):\n",
    "    ''' LSTM models structures and implementation.\n",
    "    '''\n",
    "    def __init__(self, dataset=None, model_path='', model_type='',\n",
    "                        output_dim=64, epochs=400, verbose=1,\n",
    "                        loss='mse', optimizer='adam', dropout=0.2,\n",
    "                        activation=None, share_attention=False, **args):\n",
    "        ''' Initialize the model parameters\n",
    "        param dataset: dataset Dataframe\n",
    "        param output_dim: output dimension\n",
    "        param epochs: training epochs\n",
    "        param verbose: verbose level of logging\n",
    "        param loss: loss function for training\n",
    "        param optimizer: optimizer for training\n",
    "        '''\n",
    "        super(LSTMModel, self).__init__(dataset, model_path,\n",
    "                                        output_dim, epochs, verbose,\n",
    "                                        loss, optimizer, dropout, **args)\n",
    "        self.share_attention = share_attention\n",
    "        self.activation = activation\n",
    "    \n",
    "    def attention_block(input):\n",
    "        ''' Attention block\n",
    "        '''\n",
    "        # (batch_size, time_steps, input_dim)\n",
    "        input_dim = int(input.shape[2])\n",
    "        # Transpose with Permute\n",
    "        a = Permute((2, 1))(inputs)\n",
    "        a = Reshape((input_dim, self.dataset.timestamp))(a)\n",
    "        a = Dense(self.dataset.timestamp, activation='softmax')(a)\n",
    "        if self.share_attention:\n",
    "            a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "            a = RepeatVector(input_dim)(a)\n",
    "        a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "        return Multiply()([inputs, a_probs])\n",
    "\n",
    "    def construct_single_layer_lstm(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(units=self.output_dim, input_shape=(self.dataset.timestamp, len(self.dataset.feature_cols))))\n",
    "        # self.model.add(Dropout(self.dropout))\n",
    "        self.model.add(Dense(units=len(self.dataset.label_cols)))\n",
    "        # Activation Functions\n",
    "        if self.activation != None:\n",
    "            self.model.add(Activation(self.activation))\n",
    "        # MSE loss and adam optimizer\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "\n",
    "    def construct_multi_layer_lstm(self):\n",
    "        self.model = Sequential()\n",
    "        # Three-layer LSTM with Dropout\n",
    "        self.model.add(LSTM(units=self.output_dim, return_sequences=True, input_shape=(self.dataset.timestamp, len(self.dataset.feature_cols))))\n",
    "        # self.model.add(Dropout(self.dropout))\n",
    "        self.model.add(LSTM(units=self.output_dim, return_sequences=True))\n",
    "        # self.model.add(Dropout(self.dropout))\n",
    "        self.model.add(LSTM(units=self.output_dim, return_sequences=False))\n",
    "        # self.model.add(Dropout(self.dropout))\n",
    "        # Dense Layer\n",
    "        self.model.add(Dense(units=len(self.dataset.label_cols)))\n",
    "        # Activation Functions\n",
    "        if self.activation != None:\n",
    "            self.model.add(Activation(self.activation))\n",
    "        # MSE loss and adam optimizer\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer)\n",
    "\n",
    "    def construct_attention_lstm(self):\n",
    "        inputs = Input(shape=(self.dataset.timestamp, len(self.dataset.feature_cols)))\n",
    "        attention = self.attention_block(input)\n",
    "        attention = LSTM(units=self.output_dim, return_sequences=False)(attention)\n",
    "        output = Dense(1)(attention)\n",
    "        if self.activation != None:\n",
    "            self.model.add(Activation(self.activation))\n",
    "        self.model = Model(input=[inputs], output=output)\n",
    "\n",
    "    def construct_lstm_attention(self):\n",
    "        # Inputs Layer\n",
    "        inputs = Input(shape=(self.dataset.timestamp, len(self.dataset.feature_cols)))\n",
    "        # Single LSTM Layer\n",
    "        lstm = LSTM(units=self.output_dim, return_sequences=True)(inputs)\n",
    "        # Attention Block\n",
    "        attention = attention_block(lstm)\n",
    "        # Flatten to connect with Dense Layer\n",
    "        attention = Flatten()(attention)\n",
    "        output = Dense(1)(attention)\n",
    "        if self.activation != None:\n",
    "            self.model.add(Activation(self.activation))\n",
    "        self.model = Model(input=[inputs], output=output)\n",
    "\n",
    "    def construct_model(self, model_type='single_layer_lstm'):\n",
    "        ''' Build model from scratch\n",
    "        param model_type:\n",
    "            - single_layer_lstm\n",
    "            - multi_layer_lstm\n",
    "            - attention_lstm\n",
    "            - lstm_attention\n",
    "            - transformer\n",
    "        '''\n",
    "        if model_type is 'single_layer_lstm':\n",
    "            self.construct_single_layer_lstm()\n",
    "        elif model_type is 'multi_layer_lstm':\n",
    "            self.construct_multi_layer_lstm()\n",
    "        elif model_type is 'attention_lstm':\n",
    "            self.construct_attention_lstm()\n",
    "        elif model_type is 'lstm_attention':\n",
    "            self.construct_lstm_attention()\n",
    "        else:\n",
    "            raise(f'model_type {moel_type} Not Implemented!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_CONFIGS = {\n",
    "    'dataset': None,\n",
    "    'model_path': '',\n",
    "    'output_dim': 128,\n",
    "    'epochs': 400,\n",
    "    'verbose': 2,\n",
    "    'loss': 'mse',\n",
    "    'optimizer': 'adam',\n",
    "    'dropout': 0.2,\n",
    "    'share_attention': False,\n",
    "}\n"
   ]
  },
  {
   "source": [
    "## Activation Experiment\n",
    "\n",
    "Test on 4 activation functions in LSTM, including:\n",
    "\n",
    "- sigmoid\n",
    "- tanh\n",
    "- relu\n",
    "- linear"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readning in the dataset\n",
    "dataset_csv = pd.read_csv('./data/dataset.csv', index_col=0)\n",
    "dataset_rolling_csv = pd.read_csv('./data/dataset_rolling.csv', index_col=0)\n",
    "# Construct dataset\n",
    "features = ['dates', 'people_vaccinated', 'mobility', 'hosp_patients', 'icu_patients', 'total_tests', 'median_age']\n",
    "labels = ['confirmed']\n",
    "dataset = Dataset(dataset_csv, feature_cols=features, label_cols=labels)\n",
    "dataset_rolling = Dataset(dataset_rolling_csv, feature_cols=features, label_cols=labels)\n",
    "\n",
    "\n",
    "model_types = ['single_layer_lstm', 'multi_layer_lstm', 'attention_lstm', 'lstm_attention']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start training single_layer_lstm\n",
      "Epoch 1/400\n",
      "393/393 [==============================] - 3s 4ms/step - loss: 0.0450\n",
      "Epoch 2/400\n",
      "232/393 [================>.............] - ETA: 0s - loss: 0.0074"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-ab98bb2374e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTMModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-8eebaf97d284>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m         '''\n\u001b[0;32m     31\u001b[0m         \u001b[0mtraining_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_training_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Enumerating and training\n",
    "for model_type in model_types:\n",
    "    for activation in ['sigmoid', 'tanh', 'relu', 'linear']:\n",
    "        configs = {\n",
    "            'dataset': dataset_rolling,\n",
    "            'model_path': f\"./models/{model_type}_confirmed_{activation}\",\n",
    "            'model_type': model_type,\n",
    "            'activation': activation,\n",
    "        }\n",
    "        print(f'Start training {model_type}')\n",
    "        model = LSTMModel(**configs)\n",
    "        model.construct_model()\n",
    "        model.train()\n",
    "        model.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'single_layer_lstm'\n",
    "activations = ['sigmoid', 'tanh', 'relu', 'linear']\n",
    "loss = []\n",
    "for activation in activations:\n",
    "    configs = {\n",
    "        'dataset': dataset_rolling,\n",
    "        'model_path': f\"./models/{model_type}_confirmed_{activation}\",\n",
    "        'model_type': model_type,\n",
    "        'activation': activation,\n",
    "    }\n",
    "    model = LSTMModel(**configs)\n",
    "    model.read_model()\n",
    "    # model.plot(f'Confirmed Cases Forcasting with Activation: {activation}')\n",
    "    loss.append(model.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'single_layer_lstm'\n",
    "activation = 'sigmoid'\n",
    "configs = {\n",
    "    'dataset': dataset_rolling,\n",
    "    'model_path': f\"./models/{model_type}_{'_'.join(labels)}_{activation}\",\n",
    "    'model_type': model_type,\n",
    "    'activation': activation,\n",
    "}\n",
    "model = LSTMModel(**configs)\n",
    "model.read_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type in model_types:\n",
    "    configs = {\n",
    "        'dataset': dataset,\n",
    "        'model_path': f\"./models/{model_type}_{'_'.join(labels)}\",\n",
    "        'model_type': model_type,\n",
    "    }\n",
    "    model = LSTMModel(**configs)\n",
    "    model.read_model()\n",
    "    model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type in model_types:\n",
    "    configs = {\n",
    "        'dataset': dataset,\n",
    "        'model_path': f\"./models/{model_type}_{'_'.join(labels)}\",\n",
    "        'model_type': model_type,\n",
    "    }\n",
    "    model = LSTMModel(**configs)\n",
    "    model.read_model()\n",
    "    model.plot()"
   ]
  }
 ]
}